"""
Streamlit Dashboard for Benchmark Results Visualization.

This dashboard provides an interactive interface to visualize and compare performance metrics
from the inverse problem inference benchmark. It reads parquet files generated by the benchmark
runners and displays various plots including PSNR convergence, time breakdown, and GPU usage.

Features:
- **Data Selection**: Browse and select one or multiple dataset results from the 'outputs' directory.
- **Interactive Plots**: 
    - PSNR vs Time
    - PSNR vs Iteration
    - Time Breakdown (Gradient vs Denoiser)
    - GPU Memory Usage
- **Side-by-Side Comparison**: Compare multiple result sets or different metrics side-by-side.
- **Customizable**: Choose which plots to display.

Usage:
    Run the dashboard using Streamlit:
    $ streamlit run benchmark_invprob_inference/analysis_plots/dashboard.py

Dependencies:
    - streamlit
    - pandas
    - plotly
"""

import streamlit as st
import pandas as pd
from pathlib import Path
import sys
import os
import tempfile
import plotly.graph_objects as go

# Ensure we can import from local file
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

try:
    import visualize_general_results as viz
except ImportError:
    st.error(f"Could not import visualize_general_results. Make sure it is in {current_dir}")
    st.stop()

st.set_page_config(layout="wide", page_title="Benchmark Results Dashboard")

st.title("Benchmark Results Dashboard")
st.markdown("""
This dashboard visualizes performance metrics for inverse problem inference solvers.
It allows you to compare different benchmark runs, analyzing:
- **PSNR vs Time**: Convergence speed in terms of restoration quality.
- **PSNR vs Iteration**: Convergence quality per iteration.
- **Time Breakdown**: Computational cost split between gradient and denoiser steps.
- **GPU Memory Usage**: Peak memory consumption during inference.
            
""")

# sidebar - Data Selection
st.sidebar.header("Data Selection")

repository_root = current_dir.parent
default_outputs_dir = repository_root / "outputs"

if not default_outputs_dir.exists():
    # Fallback: maybe we are in a structure where outputs is at the root?
    # e.g. if root_dir was actually what we wanted (draft/outputs)
    root_dir = current_dir.parent.parent
    if (root_dir / "outputs").exists():
        default_outputs_dir = root_dir / "outputs"
    else:
        # Try looking relative to CWD if run from root
        cwd = Path.cwd()
        if (cwd / "benchmark_invprob_inference/outputs").exists():
             default_outputs_dir = cwd / "benchmark_invprob_inference/outputs"
        elif (cwd / "outputs").exists():
            default_outputs_dir = cwd / "outputs"
        else:
            default_outputs_dir = Path("outputs") 

# Allow user to enter path
outputs_path_input = st.sidebar.text_input("Outputs Directory Path", value=str(default_outputs_dir))
outputs_path = Path(outputs_path_input)

# List subdirectories (potential results)
if outputs_path.exists() and outputs_path.is_dir():
    # List all subdirs that might contain results
    try:
        subdirs = [d.name for d in outputs_path.iterdir() if d.is_dir() and not d.name.startswith('.')]
        subdirs.sort()
        
        subdirs = [d.name for d in outputs_path.iterdir() if d.is_dir() and not d.name.startswith('.')]
        subdirs.sort()
        
        selected_results = st.sidebar.multiselect("Select Result Datasets", subdirs, default=subdirs[0] if subdirs else None)
        
        # Sidebar - Plot Selection
        st.sidebar.header("Visualization Options")
        
        # Option to choose image(s) to include
        plot_options = {
            "PSNR vs Time": viz.plot_psnr_vs_time,
            "PSNR vs Iteration": viz.plot_psnr_vs_iteration,
            "Time Breakdown": viz.plot_time_breakdown_stacked,
            "GPU Memory": viz.plot_gpu_memory_from_parquet
        }
        
        selected_plots = st.sidebar.multiselect(
            "Choose figures to include",
            list(plot_options.keys()),
            default=list(plot_options.keys())
        )

        if selected_results:
            for selected_result in selected_results:
                result_dir = outputs_path / selected_result
                
                # Check if parquet exists to avoid ugly errors from read_parquet_data
                if not list(result_dir.glob("*.parquet")):
                    st.warning(f"No .parquet files found in {result_dir}")
                    continue
                
                try:
                    with st.spinner(f"Loading data for {selected_result}..."):
                        parquet_df, result_name = viz.read_parquet_data(result_dir)
                    
                    st.sidebar.success(f"Loaded {len(parquet_df)} records for {selected_result}")
                    
                    # Main Content visualization
                    st.header(f"Results: {selected_result}")
                    
                    # Use a temporary directory for the file writing part of the functions
                    # so we don't clutter the actual result directory or overwrite things.
                    with tempfile.TemporaryDirectory() as tmpdirname:
                        tmp_path = Path(tmpdirname)
                        
                        # Create a grid for plots (2 columns)
                        cols = st.columns(2)
                        
                        for i, plot_name in enumerate(selected_plots):
                            plot_func = plot_options[plot_name]
                            
                            try:
                                # Functions expect (parquet_df, output_dir)
                                # and return fig
                                fig = plot_func(parquet_df, tmp_path)
                                
                                if fig:
                                    # Update layout to fit nicely in the grid and page
                                    fig.update_layout(
                                        height=400,  # Constrain height
                                        margin=dict(l=20, r=20, t=50, b=20),
                                        legend=dict(
                                            font=dict(size=12),  # Smaller legend font
                                            orientation="h",     # Horizontal legend to save vertical space
                                            yanchor="bottom",
                                            y=1.02,
                                            xanchor="right",
                                            x=1
                                        ),
                                        font=dict(size=12),      # Smaller overall font
                                        title=dict(font=dict(size=16)) # Smaller title
                                    )
                                    # Also update axis titles if they are huge in the original script
                                    fig.update_xaxes(title_font=dict(size=14), tickfont=dict(size=12))
                                    fig.update_yaxes(title_font=dict(size=14), tickfont=dict(size=12))

                                    with cols[i % 2]:
                                        st.plotly_chart(fig, use_container_width=True)
                                else:
                                    # Case where function might return None (e.g. GPU memory if cols missing)
                                    with cols[i % 2]:
                                        st.info(f"No plot generated for {plot_name} (check data availability)")
                                    
                            except Exception as e:
                                with cols[i % 2]:
                                    st.error(f"Error generating {plot_name}: {str(e)}")
                    
                    st.markdown("---")

                except Exception as e:
                    st.error(f"Error loading parquet data for {selected_result}: {str(e)}")
                    st.warning("Ensure the selected directory contains a valid benchmark result.")
                    
    except Exception as e:
         st.error(f"Error listing directories: {str(e)}")

else:
    st.error(f"Directory not found: {outputs_path}")
    st.info("Please provide a valid path to the 'outputs' directory containing benchmark results.")

st.sidebar.markdown("---")
st.sidebar.markdown("Generated by Dashboard Script")
